---
hide:
  - footer
---

# 5. Build the Neural Network

---

Neural networks comprise of layers/modules that perform operations on data. The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.

In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset.

```python
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
```

## Get Device for Training

We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let’s check to see if `torch.cuda` is available, else we continue to use the CPU.

=== "Code"

    ```python
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using {device} device")
    ```

=== "Out"

    ```
    Using cuda device
    ```

## Define the Class

We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method.

```python
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure.

=== "Code"

    ```python
    model = NeuralNetwork().to(device)
    print(model)
    ```

=== "Out"

    ```
    NeuralNetwork(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    )
    ```

To use the model, we pass it the input data. This executes the model’s `forward`, along with some background operations. Do not call `model.forward()` directly!

Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. . We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module.

=== "Code"

    ```python
    X = torch.rand(1, 28, 28, device=device)
    logits = model(X)
    pred_probab = nn.Softmax(dim=1)(logits)
    y_pred = pred_probab.argmax(1)
    print(f"Predicted class: {y_pred}")
    ```

=== "Out"

    ```
    Predicted class: tensor([2], device='cuda:0')
    ```

<br/>

---

## Model Layers

Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network.

=== "Code"

    ```python
    input_image = torch.rand(3,28,28)
    print(input_image.size())
    ```

=== "Out"

    ```
    torch.Size([3, 28, 28])
    ```

### `nn.Flatten`

We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).

=== "Code"

    ```python
    flatten = nn.Flatten()
    flat_image = flatten(input_image)
    print(flat_image.size())
    ```

=== "Out"

    ```
    torch.Size([3, 784])
    ```

### `nn.Linear`

The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.

=== "Code"

    ```python
    layer1 = nn.Linear(in_features=28*28, out_features=20)
    hidden1 = layer1(flat_image)
    print(hidden1.size())
    ```

=== "Out"

    ```
    torch.Size([3, 20])
    ```

### `nn.ReLU`

Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce _nonlinearity_, helping neural networks learn a wide variety of phenomena.

In this model, we use `nn.ReLU` between our linear layers, but there’s other activations to introduce non-linearity in your model.

=== "Code"

    ```python
    print(f"Before ReLU: {hidden1}\n\n")
    hidden1 = nn.ReLU()(hidden1)
    print(f"After ReLU: {hidden1}")
    ```

=== "Out"

    ```
    Before ReLU: tensor([[ 0.4988,  0.1104,  0.5095, -0.0039,  0.0648,  0.4597,  0.3565,  0.1058,
             -0.4450, -0.5919, -0.2224, -0.4621, -0.5852,  0.1055,  0.0763, -0.0396,
              0.0664, -0.0582, -0.0969, -0.3257],
            [ 0.3097, -0.2333,  0.2847, -0.0299,  0.0698,  0.2570,  0.6639,  0.2283,
             -0.4441, -0.6207,  0.0160, -0.4387, -0.6074, -0.0954,  0.5952,  0.3003,
             -0.1924,  0.0746,  0.0703, -0.1286],
            [ 0.6549,  0.2369,  0.6299,  0.0896,  0.1700,  0.1656,  0.3755,  0.1096,
             -0.2217, -0.5969, -0.5345, -0.3031, -0.6031,  0.1542,  0.2401,  0.0036,
              0.0358,  0.1442,  0.2810, -0.0572]], grad_fn=<AddmmBackward0>)


    After ReLU: tensor([[0.4988, 0.1104, 0.5095, 0.0000, 0.0648, 0.4597, 0.3565, 0.1058, 0.0000,
             0.0000, 0.0000, 0.0000, 0.0000, 0.1055, 0.0763, 0.0000, 0.0664, 0.0000,
             0.0000, 0.0000],
            [0.3097, 0.0000, 0.2847, 0.0000, 0.0698, 0.2570, 0.6639, 0.2283, 0.0000,
             0.0000, 0.0160, 0.0000, 0.0000, 0.0000, 0.5952, 0.3003, 0.0000, 0.0746,
             0.0703, 0.0000],
            [0.6549, 0.2369, 0.6299, 0.0896, 0.1700, 0.1656, 0.3755, 0.1096, 0.0000,
             0.0000, 0.0000, 0.0000, 0.0000, 0.1542, 0.2401, 0.0036, 0.0358, 0.1442,
             0.2810, 0.0000]], grad_fn=<ReluBackward0>)
    ```

### `nn.Sequential`

`nn.Sequential` is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`.

```python
seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
```

### `nn.Softmax`

The last linear layer of the neural network returns _logits_ - raw values in [-infty, infty] - which are passed to the `nn.Softmax` module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. `dim` parameter indicates the dimension along which the values must sum to 1.

```python
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
```

## Model Parameters

Many layers inside a neural network are _parameterized_, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.

In this example, we iterate over each parameter, and print its size and a preview of its values.

=== "Code"

    ```python
    print(f"Model structure: {model}\n\n")

    for name, param in model.named_parameters():
        print(f"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n")
    ```

=== "Out"

    ```
    Model structure: NeuralNetwork(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    )


    Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0229, -0.0028, -0.0241,  ...,  0.0319, -0.0086, -0.0207],
            [ 0.0155, -0.0026,  0.0321,  ..., -0.0103,  0.0049, -0.0344]],
           device='cuda:0', grad_fn=<SliceBackward0>)

    Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0283, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)

    Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0234,  0.0337, -0.0222,  ..., -0.0040,  0.0032,  0.0279],
            [ 0.0397, -0.0134, -0.0293,  ...,  0.0131, -0.0424,  0.0343]],
           device='cuda:0', grad_fn=<SliceBackward0>)

    Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0333, 0.0191], device='cuda:0', grad_fn=<SliceBackward0>)

    Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0021, -0.0263,  0.0034,  ...,  0.0250, -0.0218,  0.0266],
            [-0.0177,  0.0311, -0.0379,  ..., -0.0167,  0.0137, -0.0286]],
           device='cuda:0', grad_fn=<SliceBackward0>)

    Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0157, -0.0078], device='cuda:0', grad_fn=<SliceBackward0>)
    ```

---

## References

- [https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)
