---
hide:
  - footer
---

# 5. Context-Dependent AM

---

!!! info

    주변 음소 정보까지 모델링에 고려하는 Context-Dependent Acoustic Model을 살펴본다.

<br/>

## 1. Motivation

**그림1**은 영어 모음(vowel) `eh`의 스펙트럼(spectrum)을 나타낸 것이다. 첫 번째는 `WED`의 `eh`, 두 번째는 `YELL`의 `eh`, 세 번째는 `BEN`의 `eh`이다. 같은 모음이지만 앞뒤(context)에 어떤 음소가 오느냐에 따라 그 특질이 확연하게 달라짐을 확인할 수 있다. 기존 히든 마코프 모델(Hidden Markov Model) 기반 음성 인식 모델에서는 상태(state)를 음소보다 작은 단위의 subphone으로 모델링하고 있는데, **그림1**과 같이 동일한 음소라도 그 특징이 크게 다르다면 인식 품질이 확 낮아지게 될 것이다.

<br/>

!!! note "**그림1** CD PHONE MOTIVATION"

    <figure markdown>
      ![001](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/001.png?raw=true){ load=lazy }
    </figure>

<br/>

이에 도입된 개념이 **Context Dependent Phone(CD Phone)**이다. 앞뒤 컨텍스트를 반영하는 것이다. 예컨대 `YELL`의 `eh`를 보자. 이 `eh`는 그 이전에 `y`, 그 이후에 `l`이 등장한다. 이에 `YELL`의 `eh`를 `y-eh+l`로 표현하게 된다. 마찬가지로 `WED`의 `eh`는 `e-eh+d`, `BEN`의 `eh`는 `b-eh+n`으로 표시해 `YELL`의 `eh`와 구별한다. CD Phone을 모델링할 때 기준 음소를 포함해 그 앞뒤 음소 총 3개를 고려하는 triphone을 일반적으로 사용한다.

<br/>

CD Phone은 subphone과는 대비되는 개념이다. CD Phone 모델링 시 음소보다 작은 단위인 subphone을 쓸 수 있지만 그 자체로 CD Phone이 되는 것은 아니다. CD Phone을 모델링한다는 것은 '음소보다 작은 단위를 쓴다'에 강조점이 있는 것이 아니라 '앞뒤 컨텍스트를 고려한다'에 방점이 찍혀 있기 때문이다. 한편 CD Phone의 대척점에 Context-Independent Phone(혹은 monophone)이 있다. 음소 모델링 시 앞뒤 컨텍스트르 고려하지 않고 독립적으로 본다는 뜻이다.

<br/>

## 2. Subphone Clustering

문제는 이렇게 CD Phone을 상정하게 되면 고려해야 하는 경우의 수가 폭증한다는 점에 있다. 50개 음소가 있는 언어이고 CD Phone을 기준 음소와 그 앞뒤 음소 총 3개(triphone)으로 모델링한다면 우리는 $50^{3}=125000$개의 CI Phone을 염두에 두어야 한다. 이에 **그림2**의 `iy`와 같이 비슷한 특징을 가지는 CD Phone을 하나로 합쳐 고려하는 **Subphone Clustering**이 제시됐다. 아래처럼 비슷한 CD Phone은 동일한 상태(state)로 간주해 계산량을 줄이는 기법이다.

<br/>

!!! note "**그림2** SUBPHONE CLUSTERING MOTIVATION"

    <figure markdown>
      ![002](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/002.png?raw=true){ load=lazy }
    </figure>

<br/>

기존 음성 인식 시스템에서는 상태(state)가 주어졌을 때 관측치(observation)가 나타날 확률, 즉 방출(emission) 확률 함수를 가우시안 믹스처 모델(Gaussian Mixture Model)로 모델링한다. 상태 개수만큼의 가우시안 믹스처 모델이 필요하다. 앞서 언급했듯 CD Phone을 상정하게 되면 학습해야 하는 가우시안 믹스처 모델이 기하급수적으로 많아지게 된다. Subphone Clustering을 실시하게 되면 비슷한 특질을 가지는 CD Phone을 묶을(tying) 수 있고 가우시안 믹스처 모델 역시 공유할 수 있게 된다. **그림3**과 같다.

<br/>

!!! note "**그림3** SUBPHONE CLUSTERING"

    <figure markdown>
      ![003](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/003.png?raw=true){ load=lazy }
    </figure>

<br/>

**그림4**는 `iy`를 CD Phone으로 모델링할 때 클러스터링 절차를 도식적으로 나타낸 것이다. (1) 처음에는 `iy`를 Context-Independent Phone이라고 보고 히든 마코프 모델 + 가우시안 믹스처 모델을 학습한다. (2) `iy`를 CD Phone으로 모델링한 히든 마코프 모델 + 가우시안 믹스처 모델을 구축하되, `iy`에 관련된 모든 CD Phone에 대응되는 가우시안 믹스처 모델의 초기값으로 (1)에서 학습한 가우시안 믹스처 모델의 파라미터(평균, 공분산)을 준다.

<br/>

!!! note "**그림4** CLUSTERING PROCESS"

    <figure markdown>
      ![004](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/004.png?raw=true){ load=lazy }
    </figure>

<br/>

(3)에서는 비슷한 특질을 가지는 CD Phone끼리 클러스터링을 수행하고 그 결과에 따라 가우시안 믹스처 모델 파라미터를 공유할지(tying) 말지 결정해 준다. 마지막으로 (4)에서는 (3) 결과를 바탕으로 히든 마코프 모델 + 가우시안 믹스처 모델을 다시 학습한다.

<br/>

## 3. Decision Tree

Subphone Clustering에 사용되는 군집화 알고리즘은 바로 의사결정나무(Decision Tree)이다. 음성 피처(MFCC)가 주어졌을 때 히든 마코프 모델 + 가우시안 믹스처 모델의 가능도(likelihood)가 높아지면 분기(split)하도록 학습한다. **그림5**는 이렇게 학습된 의사결정나무 모델의 예시를 나타낸 그림이다.

<br/>

!!! note "**그림5** DECISION TREE"

    <figure markdown>
      ![005](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/005.png?raw=true){ load=lazy }
    </figure>

<br/>

의사결정나무 분기 조건은 **그림6**과 같이 음운론, 음성학 전문가들이 추출해 놓은 음성 특징을 기준으로 한다.

<br/>

!!! note "**그림6** PHONETIC FEATURES"

    <figure markdown>
      ![006](https://github.com/SAEMC/Images-MLDL/blob/main/speech-book/ch-004/005/006.png?raw=true){ load=lazy }
    </figure>

---

## References

- [https://ratsgo.github.io/speechbook/docs/am/cdam](https://ratsgo.github.io/speechbook/docs/am/cdam)
